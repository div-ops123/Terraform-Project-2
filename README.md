### Project Overview: KubeStack Automation

**Disclaimer**: Application code by Author: Ryan Almeida
ðŸ“Œ GitHub Profile: Ryan Almeida[https://github.com/ryan-almeida]

**Goal**: Provision a scalable, containerized note-taking app on Kubernetes, deployed on EC2 instances using Terraform for infrastructure, Ansible for configuration management, and a CI/CD pipeline (Jenkins + Harness) for automation, using Terraform modules for networking and compute, and remote state in S3/DynamoDB.

**Components**:
- **Infrastructure (Terraform)**:
  - VPC, subnets, security groups
  - EC2 instances for Kubernetes nodes
  - Auto Scaling Group (ASG) for worker nodes
  - Application Load Balancer (ALB)
  - S3 bucket and DynamoDB table for Terraform state

- **Configuration Management (Ansible)**:
  - Kubernetes installation and node setup
  - Application deployment configurations

- **Orchestration (Kubernetes)**:
  - Node.js app running as a containerized service
  - Ingress controller for routing

- **CI/CD (Jenkins + Harness)**:
  - Jenkins pipeline for building and testing
  - Harness for automated deployments

- **State**: 
  - S3 bucket and DynamoDB table for Terraform state.

---

### Project Structure
Following best practices:

```
./
â”œâ”€â”€ application-code/          # Node.js note-app source code
â”‚   â”œâ”€â”€ src/                  # App source files (e.g., server.js)
â”‚   â”œâ”€â”€ package.json          # Node.js dependencies and scripts
â”‚   â”œâ”€â”€ Dockerfile            # Docker image definition for the app
â”‚   â””â”€â”€ .dockerignore         # Ignore files for Docker build
â”œâ”€â”€ terraform/                # Terraform infra code
â”‚   â”œâ”€â”€ main.tf               # Root module: Calls networking and compute modules
â”‚   â”œâ”€â”€ variables.tf          # Root-level variables
â”‚   â”œâ”€â”€ outputs.tf            # Root-level outputs (e.g., cluster IPs)
â”‚   â”œâ”€â”€ provider.tf           # AWS provider config
â”‚   â”œâ”€â”€ versions.tf           # Terraform and provider versions, S3 backend
â”‚   â”œâ”€â”€ bootstrap/            # Bootstrap module for remote state management
â”‚   â”‚   â”œâ”€â”€ main.tf           # Manages S3 backend and DynamoDB for state locking
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ networking/       # VPC, subnets, IGW, etc.
â”‚       â”‚   â”œâ”€â”€ main.tf
â”‚       â”‚   â”œâ”€â”€ variables.tf
â”‚       â”‚   â””â”€â”€ outputs.tf
â”‚       â””â”€â”€ compute/          # EC2 instances for Kubernetes nodes
â”‚           â”œâ”€â”€ main.tf
â”‚           â”œâ”€â”€ variables.tf
â”‚           â””â”€â”€ outputs.tf
â”œâ”€â”€ ansible/                  # Ansible config for Kubernetes setup
â”‚   â”œâ”€â”€ inventory.yml         # List of EC2 instances (dynamically populated)
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”‚   â”œâ”€â”€ install-k8s.yml   # Install Kubernetes (kubeadm, Docker, etc.)
â”‚   â”‚   â””â”€â”€ configure-cluster.yml  # Join nodes to the cluster
â”‚   â””â”€â”€ roles/                # Reusable Ansible roles (optional)
â”‚       â”œâ”€â”€ kubernetes/
â”‚       â”‚   â”œâ”€â”€ tasks/
â”‚       â”‚   â”œâ”€â”€ templates/
â”‚       â”‚   â””â”€â”€ vars/
â”œâ”€â”€ kubernetes/               # Kubernetes manifests for the note-app
â”‚   â”œâ”€â”€ deployment.yml        # Deployment for the Node.js app
â”‚   â”œâ”€â”€ service.yml           # Service to expose the app (e.g., LoadBalancer)
â”‚   â””â”€â”€ configmap.yml         # Optional: Config for the app (if needed)
â”œâ”€â”€ .gitignore                # Ignore transient files
â””â”€â”€ README.md                 # Project overview and setup instructions
```

#### Prerequisites

### aws, aws cli, and terraform installed and set up

#### Deployment Guide

## A. Configuring Remote State Management with S3 and DynamoDB

**Run the Bootstrap**:
1. Navigate to the `bootstrap/` directory:
   ```bash
   cd bootstrap
   ```
2. Initialize and apply:
   ```bash
   terraform init
   terraform apply
   ```
   - Type `yes` when prompted to create the resources.
3. This creates the S3 bucket and DynamoDB table, storing the state locally in `bootstrap/terraform.tfstate`.

**Initialize the Remote Backend**:
1. Navigate to your main project directory

2. Run:
   ```bash
   terraform init
   ```
   - Terraform will detect the backend configuration and prompt you to migrate any existing local state to S3. Type `yes`.

### Why This Works
- The bootstrap step creates the S3 bucket and DynamoDB table independently.
- The main project then uses these resources to store its state remotely, with DynamoDB providing state locking to prevent concurrent modifications.


## B. 

`application-code/`
- Test locally: `docker build -t note-app . && docker run -p 3000:3000 note-app`.


`terraform/`
- It can assign static private IPs to avoid changes when restarting.
ðŸ”¹ If you stop & start VMs, the IPs remain the same, and the cluster continues to work.
- Output public IPs for Ansible inventory.


`ansible/`
- Use Ansible roles for modularity (e.g., `kubernetes` role).
- Test playbook locally first with a single VM if possible.


`kubernetes/`
- Use `kubectl apply -f kubernetes/` to deploy manually first.


CI/CD (Jenkins + Harness)
- **CI (Jenkins)**:
  - **Pipeline**: `Jenkinsfile` to `npm test`, `docker build`, `docker push` to a registry (e.g., Docker Hub).
- **CD (Harness)**:

  - **Setup**: Install Jenkins on a separate EC2 or locally.
  - Start with manual `kubectl` deployment, then automate with Jenkins/Harness.

---

### Next Steps
- **Start Small**: Focus on `terraform/` firstâ€”update `compute` module for EC2 instances.
modules/networking/main.tf
- **Then**: Move to `ansible/` for Kubernetes setup.
- **Later**: Tackle `kubernetes/` and CI/CD.

---

### Workflow Steps
1. **Terraform**:
   - Provision VPC, subnets, and 3 EC2 instances (1 master, 2 workers).
   - Output IPs for Ansible.
2. **Ansible**:
   - Run `install-k8s.yml` to install Kubernetes.
   - Run `configure-cluster.yml` to set up the cluster.
3. **CI (Jenkins)**:
   - Build Docker image from `application-code/`.
   - Push to a registry.
4. **Kubernetes**:
   - Deploy the image manually with `kubectl` to test.
5. **CD (Harness)**:
   - Automate deployment to Kubernetes.

---